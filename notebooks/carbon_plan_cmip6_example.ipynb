{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c6941701",
   "metadata": {},
   "source": [
    "# Carbon Plan CMIP6 downscaling example\n",
    "\n",
    "In this notebook I'll show you a simple way to access carbonplan's downscaled CMIP6 data. This takes heavy inspiration from their notebook, [here](https://github.com/carbonplan/cmip6-downscaling/blob/main/notebooks/accessing_data_example.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55952b41",
   "metadata": {},
   "source": [
    "The first thing we're going to do is import the relevant python libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d33c3884",
   "metadata": {},
   "outputs": [],
   "source": [
    "import intake\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pdb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3700ede3",
   "metadata": {},
   "source": [
    "Now let's open the Carbon Plan datastore as a catalogue, so that I can subset it for the data I need:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6fdb876f",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat = intake.open_esm_datastore(\n",
    "    \"https://cpdataeuwest.blob.core.windows.net/cp-cmip/version1/catalogs/global-downscaled-cmip6.json\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42f6fd06",
   "metadata": {},
   "source": [
    "In the next section I'm looking at what data is available in the entire catalogue:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b1b08954",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "cat_df = cat.df #First convert the catalogue to a pandas dataframe\n",
    "\n",
    "#Look for unique entries in each of the columns of the dataframe\n",
    "\n",
    "models_avail = np.unique([key for key in cat_df['source_id']])\n",
    "\n",
    "methods_avail = np.unique([key for key in cat_df['method']])\n",
    "\n",
    "exps_avail = np.unique([key for key in cat_df['experiment_id']])\n",
    "\n",
    "vars_avail = np.unique([key for key in cat_df['variable_id']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ce12b75",
   "metadata": {},
   "source": [
    "Now I have an idea of all the unique models that are available, all the methods etc. But it's going to turn out that not all the downscaling methods are available for each of the models, so we have to check what's available. To do that, I'm going to use my list of models to check what's available for each of them, and print out the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "75c4a5d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For the BCC-CSM2-MR climate model, the following data is available:\n",
      "The downscaling methods available are ['GARD-SV']\n",
      "The experiments that are available are ['historical' 'ssp245' 'ssp370' 'ssp585']\n",
      "The downscaled variables that are available are ['tasmax' 'tasmin']\n",
      "\n",
      "\n",
      "For the CanESM5 climate model, the following data is available:\n",
      "The downscaling methods available are ['DeepSD' 'DeepSD-BC' 'GARD-SV']\n",
      "The experiments that are available are ['historical' 'ssp245' 'ssp370' 'ssp585']\n",
      "The downscaled variables that are available are ['pr' 'tasmax' 'tasmin']\n",
      "\n",
      "\n",
      "For the MIROC6 climate model, the following data is available:\n",
      "The downscaling methods available are ['GARD-SV']\n",
      "The experiments that are available are ['historical' 'ssp245' 'ssp370' 'ssp585']\n",
      "The downscaled variables that are available are ['pr' 'tasmax' 'tasmin']\n",
      "\n",
      "\n",
      "For the MPI-ESM1-2-HR climate model, the following data is available:\n",
      "The downscaling methods available are ['GARD-SV']\n",
      "The experiments that are available are ['historical' 'ssp245' 'ssp370' 'ssp585']\n",
      "The downscaled variables that are available are ['pr' 'tasmax' 'tasmin']\n",
      "\n",
      "\n",
      "For the MRI-ESM2-0 climate model, the following data is available:\n",
      "The downscaling methods available are ['DeepSD' 'DeepSD-BC' 'GARD-MV' 'GARD-SV' 'MACA']\n",
      "The experiments that are available are ['historical' 'ssp245' 'ssp370' 'ssp585']\n",
      "The downscaled variables that are available are ['pr' 'tasmax' 'tasmin']\n",
      "\n",
      "\n",
      "For the NorESM2-LM climate model, the following data is available:\n",
      "The downscaling methods available are ['GARD-MV' 'MACA']\n",
      "The experiments that are available are ['historical' 'ssp245']\n",
      "The downscaled variables that are available are ['pr' 'tasmax' 'tasmin']\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Loop over each of the models available\n",
    "for model_to_check in models_avail:\n",
    "\n",
    "    #Search the catalogue for all the data related to that specific model\n",
    "    cat_subset = cat.search(\n",
    "        source_id = model_to_check,\n",
    "    )\n",
    "\n",
    "    #Turn it into a pandas dataframe again\n",
    "    cat_subset_df = cat_subset.df\n",
    "\n",
    "    #Summarise the data available for that model:\n",
    "\n",
    "    methods_avail_subs = np.unique([key for key in cat_subset_df['method']])\n",
    "    exps_avail_subs = np.unique([key for key in cat_subset_df['experiment_id']])\n",
    "    vars_avail_subs = np.unique([key for key in cat_subset_df['variable_id']])\n",
    "\n",
    "    print(f'For the {model_to_check} climate model, the following data is available:') \n",
    "    print(f'The downscaling methods available are {methods_avail_subs}')\n",
    "    print(f'The experiments that are available are {exps_avail_subs}')\n",
    "    print(f'The downscaled variables that are available are {vars_avail_subs}')\n",
    "    print(f'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3d1e625",
   "metadata": {},
   "source": [
    "Now that we know what's available, let's choose a specific model, specific downscaling method, a specific experiment and a specfic variable to look at."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c20f07c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_to_search = 'MIROC6' #This is the name of the climate model\n",
    "method_to_search = 'GARD-SV' #This is the downscaling method to be used\n",
    "experiment_to_search = 'historical' #This is the name of the CMIP6 experiment I want to look at\n",
    "variable_to_search = 'tasmax' #This is the name of the variable I want to look at, e.g. tasmax, which is the maximum surface air temperature."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dc89335",
   "metadata": {},
   "source": [
    "In this project we're interested in how the weather changes at the locations of our sporting events. With that in mind, let's choose the latitude and longitude of our event, and the season we want to study (be careful when considering southern hemisphere vs northern hemisphere!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f2c7fc44",
   "metadata": {},
   "outputs": [],
   "source": [
    "latitude_of_location = 50.7260\n",
    "longitude_of_location = -3.5275\n",
    "season_to_study ='JJA'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b49a3a58",
   "metadata": {},
   "source": [
    "Now that I've made all these specifications, let's begin by searching the catalogue for data that match my criteria. Note that if you comment out any of the lines in the following then you'll see more datasets. E.g. if you comment out the line below specifying `experiment_id=experiment_to_search` then the catalogue search will return all the experiments that are available, rather than just one. This might be useful to you later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "a3f76d1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--> The keys in the returned dictionary of datasets are constructed as follows:\n",
      "\t'activity_id.institution_id.source_id.experiment_id.timescale.method'\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      <progress value='1' class='' max='1' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      100.00% [1/1 00:00&lt;00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Search the catalogue for my search criteria\n",
    "cat_search_subset = cat.search(\n",
    "    source_id=model_to_search, \n",
    "    method=method_to_search, \n",
    "    experiment_id=experiment_to_search, \n",
    "    variable_id=variable_to_search,\n",
    ")\n",
    "\n",
    "#Turn my catalogue search into xarray datasets\n",
    "dsets = cat_search_subset.to_dataset_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "321afbe4",
   "metadata": {},
   "source": [
    "Sometimes I might make a search that doesn't correspond to any data (e.g. if I choose a downscaling method that isn't available for the climate model I've chosen. Let's check if that's the case or not:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c6afa147",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You have found 1 datasets that match your search criteria. These are:\n",
      "CMIP.MIROC.MIROC6.historical.day.GARD-SV \n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Perform a check to see if I have any results that match my search.\n",
    "if len(dsets)==0:\n",
    "    raise NotImplementedError('The catalogue search that you specified has returned zero results. One reason for this could be that you have mis-spelled a variable name somewhere, or that the data does not exist in the archive')\n",
    "else:\n",
    "    print(f'You have found {len(dsets)} datasets that match your search criteria. These are:')\n",
    "    for dset_name in dsets.keys():\n",
    "        print(f'{dset_name} \\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fe531df",
   "metadata": {},
   "source": [
    "Now for the intensive part. Now that I've made all of my specifications, it's time to download the data and put it into a csv file for me to analyse. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "d278e58a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File sucessfully written with name CMIP.MIROC.MIROC6.historical.day.GARD-SV_50.73_-3.53_JJA.csv\n"
     ]
    }
   ],
   "source": [
    "#Loop over each dataset in my list to find a timeseries at that location, subset it for summer\n",
    "#and write it out to a csv file.\n",
    "\n",
    "for dset_name in dsets.keys():\n",
    "\n",
    "    ds = dsets[dset_name] #open the relevant dataset\n",
    "\n",
    "    exp_id_of_ds = ds.attrs['experiment_id'] #find out which experiement it is\n",
    "\n",
    "    member_to_use = ds['member_id'].values[0] #Always look at the first ensemble member\n",
    "\n",
    "    #Actually subset the data based on location and ensemble member\n",
    "    ds_at_location = ds[variable_to_search].sel(lat=latitude_of_location, lon=longitude_of_location, method='nearest').sel(member_id=member_to_use)\n",
    "\n",
    "\n",
    "    #Specify the time periods I want to look at\n",
    "    if exp_id_of_ds=='historical':\n",
    "        #If using historical data then just look at whole thing\n",
    "        time_slice = ds_at_location.sel(time=slice('1950-01-01', '2000-12-31'))\n",
    "    else:\n",
    "        #If using a climate change scenario then look at the end of the century        \n",
    "        time_slice = ds_at_location.sel(time=slice('2080-01-01', '2099-12-31'))\n",
    "\n",
    "    #Select only the summer months (JJA in the northern hemisphere)\n",
    "    time_slice_summer = time_slice.where(time_slice.time.dt.season==season_to_study, drop=True)\n",
    "\n",
    "    #Specify the name of my output file\n",
    "    filename_out = f'{dset_name}_{latitude_of_location:.2f}_{longitude_of_location:.2f}_{season_to_study}.csv'\n",
    "\n",
    "    #Send it to a csv file\n",
    "    time_slice_summer.to_dataframe().to_csv(filename_out)\n",
    "    print(f'File sucessfully written with name {filename_out}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "116eda8d",
   "metadata": {},
   "source": [
    "Now that this is all done, I should find there's a file or set of files that's just been written that I can download and analyse using python or R or whatever you choose to use."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
